\documentclass{article}
\title{Posterior Regression: Approximated Statistically Approximate Distributions}
\author{Prasad N R\\Alumnus, Computer Engineering, \\Carnegie Mellon University\\prasadnr606@yahoo.in}
\date{7 December 2023}

\begin{document}
	\maketitle 
	\begin{abstract}
		Explainable AI is an important aspect. But, to the best of knowledge, there isn't any end-to-end probabilistic explainable algorithms that is easy to work with. So, using Laplace smoothing idea of Bayes probabilities, an end-to-end probabilistic deep-learning algorithm is proposed. Also, there are three initial ideas related to the machine learning training process of this deep-learning.\\\\
		Software-program: https://github.com/PrasadNR/PRASAD
	\end{abstract}
	
	\begin{section}{Introduction}
		Bayes probability rule provides us the posteriori. So, an idea is to use probabilities that are not dependent on specific probability distributions, Gaussian distribution for example. Also, usually Laplace smoothing is used and 1's are added to the Bayes conditional probabilities. But, this 1 may not be the best additive correction all the time. Also, a hierarchy of these probabilities is proposed. Regarding the probability training process, there seems to be none as this Approximated Bayes Approximate Distributions (ABAD) is an idea that is being proposed in this research-paper. 
	\end{section}
	
	\begin{section}{Prior Work}
		This work is inspired by Bayes statistics, Laplace smoothing and additive smoothing. Some of the convex optimisation solutions include Backpropagation and Blind-descent.\cite{hinton}\cite{blindDescent} Some of the ideas of explainable deep-learning is similar to the explanation algorithms of ABAD.\cite{explainableDeepLearning} Also, PyTorch is used for `autograd` of ABAD statistics.\cite{PyTorch} Although Scikit-learn has MNIST data-set and common functions, for ABAD, a custom Python forward-function has been designed.\cite{scikit-learn}
	\end{section}
	
	\begin{section}{Approximated Bayes Approximate Distributions (ABAD)}
		\begin{equation}
			P(X_{vector}, Y) = [\prod_i P(\frac{X_i}{Y})]P(Y) \qquad \forall_i P(\frac{X_i}{Y}) \ne 0
		\end{equation}
		"Laplace smoothing": For any i, if $P(\frac{X_i}{Y}) = 0$,
		\begin{equation}
			P(X_{vector}, Y) = [\prod_i [P(\frac{X_i}{Y}) + 1]]P(Y)
		\end{equation}

		But, this approximation of 1 may not always work if we assume Y to be a latent label. So, we can assume $w_i$ instead of 1. So, if we try to estimate this joint probability,
		\begin{equation}
			P(X_{vector}, Y) = [\prod_i [P(\frac{X_i}{Y}) + w_i]]P(Y) 
		\end{equation}
		
		\begin{equation}
			P(X_{vector}, Y) \propto P(\frac{Y}{X_{vector}}) \Rightarrow P(\frac{Y}{X_{vector}}) \propto [\prod_i [P(\frac{X_i}{Y}) + w_i]]P(Y)
		\end{equation}

		So, we have layer's Joint Probability output for X [with X as input].
		\begin{equation}
			 P(\frac{Y}{X_{vector}}) \propto \prod_i [P(\frac{X_i}{Y}) + w_i] \Rightarrow Joint \; probability \; output = \prod_i (x_i + w_i)
		\end{equation}
		We ignore P(Y) as it is a multiplicative latent variable (scaling factor). So, we can consider a "deep-learning" stack of these non-linear joint probability layers. If we consider $x_i$ to be static $\forall i$, we do not need any activation layer as 

		$$\prod_i (x_i + w_i) = \left\{
			\begin{array}{lr}
				x_i & \; if \; w_j = -x_j + 1 \;\forall j \ne i \;\&\; w_i = 0 \\
				0 & \; if \; w_i = -x_i
			\end{array}
			\right.$$ 

		But, $x_i$ "doesn't have to be" "static". So, we need an activation layer also. I use ReLU for this.\\\\
		Static $\rightarrow x_i$ is a constant for the entire batch $\forall i$.\\
		
		But, joint-probability output is an input for the next joint-probability layer. So, if we have an activation layer like ReLU between these layers, we can split the additive weight $W_{activation}$ across ReLU. So, input of ReLU is supposed to have an additive weight "B" along with the output of joint-probability layer.
		
		\begin{equation}
			Layer \; output \; = ReLU[(\prod_i(x_i + w_i)) + B]
		\end{equation}

	\end{section}

	\begin{section}{Local Approximation Theorem}
		\begin{equation}
			Layer \; output \; = ReLU[(\prod_i(x_i + w_i)) + B]
		\end{equation}
		Practically, this evaluates polynomial vectors instead of linear functions.
		
		\begin{subsection}{Assumption 1}
			x $\sim$ U(-$\epsilon$, $\epsilon$); $\epsilon \; \rightarrow \; 0$\\
			x $\sim$ Identical and Independent Uniform Random Distribution
		\end{subsection}
		
		\begin{subsection}{Assumption 2}
			$|x| << |w|$ (or $|x_i| << |w_i| \quad \forall i$)
		\end{subsection}
		
		\begin{subsection}{Assumption 3}
			From assumption 2,
			Layer output $\simeq$ ReLU[($\prod_j w_j + \Sigma_i [\prod_{j \ne i} w_j] x_i$) + B]
		\end{subsection}

		\begin{subsection}{Assumption 4}
			$w_i \ne 0 \quad \forall i$
		\end{subsection}

		\begin{subsection}{Local Approximation Theorem Proof}
			Let us assume
			\begin{equation}
				ReLU[\Sigma_i m_i x_i + b] \simeq ReLU[(\prod_j w_j + \Sigma_i[\prod_{j \ne i} w_j]x_i) + B]
			\end{equation}
			
			\begin{equation}
				\Sigma_i m_i x_i + b \simeq (\prod_j w_j + \Sigma_i[\prod_{j \ne i} w_j]x_i) + B
			\end{equation}
			
			\framebox{$m_i \simeq \prod_{j \ne i} w_j \simeq \frac{\prod_j w_j}{w_i} \qquad and \qquad b \simeq \prod_j w_j + B$}
		\end{subsection}
	\end{section}
	
	\begin{section}{Local Convex Optimisation and Discussion}
		There are many beautiful thought processes related to this. Those include faster training (as we are training polynomials instead of some linear approximations) and weight correlation (as each $x_i$ depends on all other weights).
		
		\begin{subsection}{Backpropagation}
			As each $x_i$ depends on all other weights, the sum of these gradients are important. Weights with greater magnitude are supposed to be updated with "correction" of greater magnitude.
			\begin{equation}
				\frac{\partial L}{\partial w_i} = \eta w_i \Sigma_{j \ne i} \frac{\partial L}{\partial m_j}
			\end{equation}
		\end{subsection}

		\begin{subsection}{autograd}
			For a 32-bit computer, Bias or any input that is like a bias can cause gradient magnitude problems. It is difficult to train relatively large networks of ABAD using this.
		\end{subsection}

		\begin{subsection}{Blind Descent}
			Bias wasn't considered for this as the range of the bias can be $[-\infty, \infty]$. This caused significant problems and didn't work properly.
		\end{subsection}

	\end{section}
	
	\begin{thebibliography}{1}
		
		\bibitem{blindDescent}
		Akshat Gupta and Prasad N R, \textit{“Blind Descent: A Prequel to Gradient Descent”}, Vol. 783 Lecture Notes in Electrical Engineering,
		ICDSMLA, Springer (2020)
		
		\bibitem{hinton}
		David E. Rumelhart, Geoffrey E. Hinton, Ronald J. Williams,
		\textit{Learning representations by back-propagating errors}, Vol. 323,
		Nature, 1986
		
		\bibitem{explainableDeepLearning}
		Gabrielle Ras, Ning Xie, Marcel Van Gerven and Derek Doran, \textit{"Explainable Deep Learning: A Field Guide for the Uninitiated"}, Journal of Artificial Intelligence Research [Vol. 73] (2022)
		
		\bibitem{PyTorch}
		Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Kopf, A., Yang, E., DeVito, Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L., Bai, J., and Chintala, S. (2019). \textit{PyTorch: An Imperative Style, High-Performance Deep Learning Library [Conference paper].} Advances in Neural Information Processing Systems 32, 8024–8035. http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf
	
		\bibitem{scikit-learn}
		Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
		and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
		and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
		Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E., \textit{Scikit-learn: Machine Learning in Python},
		Journal of Machine Learning Research, volume 12, pages 2825--2830, year 2011
	
	\end{thebibliography}
\end{document}
